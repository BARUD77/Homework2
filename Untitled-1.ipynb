{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafeebc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c84961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Llama-3-70B triplet generation via vLLM (OpenAI-compatible)\n",
    "# ============================================\n",
    "import os, json, math, random, time, collections, datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import jsonlines\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential_jitter\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    !pip -q install openai==1.51.2\n",
    "    from openai import OpenAI\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "# Local vLLM server (started in step 1)\n",
    "BASE_URL   = \"http://127.0.0.1:8000/v1\"\n",
    "API_KEY    = \"EMPTY\"  # vLLM ignores, but the client expects a string\n",
    "MODEL_NAME = \"<hf_model_id>\"   # same as you passed to vLLM; or --served-model-name\n",
    "\n",
    "# Target accounting (you already produced 73 from InternVL)\n",
    "TARGET_TOTAL = 2000\n",
    "ALREADY_HAVE = 73\n",
    "REMAINING    = TARGET_TOTAL - ALREADY_HAVE  # 1927\n",
    "\n",
    "# Papers to skip this run\n",
    "SKIP_SUBSTRINGS = [\"InternVL\"]\n",
    "\n",
    "# Concurrency\n",
    "MAX_WORKERS = 8\n",
    "RNG_SEED    = 13\n",
    "random.seed(RNG_SEED)\n",
    "\n",
    "# Triplet policy (per chunk)\n",
    "N_TRIPLETS_PER_CHUNK = 1           # predictable volume & cost\n",
    "\n",
    "# Paths (adjust if needed)\n",
    "CHUNKS_DIR = Path(\"data/chunks\")   # folder with *.chunks.jsonl created earlier\n",
    "OUT_DIR    = Path(\"outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Output file for this run\n",
    "ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "triplets_path = OUT_DIR / f\"triplets_run_llama70b_{ts}.jsonl\"\n",
    "\n",
    "# Prompts (same structure as your GPT-5 pipeline)\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a meticulous data constructor. Given a technical passage (CHUNK), \"\n",
    "    \"produce instruction-tuning triplets that are useful for training. \"\n",
    "    \"Prefer concrete, unambiguous, domain-grounded questions.\"\n",
    ")\n",
    "GEN_PROMPT = (\n",
    "    \"You will receive a technical passage (CHUNK). Produce {k} high-quality instruction-tuning triplets.\\n\"\n",
    "    'Each triplet is a JSON object with fields: \"question\", \"input\", \"response\".\\n'\n",
    "    \"Return a JSON array of triplets only.\"\n",
    ")\n",
    "\n",
    "# OpenAI-compatible client pointed at vLLM\n",
    "client = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "\n",
    "# -----------------------\n",
    "# DISCOVER ELIGIBLE PAPERS\n",
    "# -----------------------\n",
    "def skip_file(p: Path) -> bool:\n",
    "    name = p.name.lower()\n",
    "    return any(s.lower() in name for s in SKIP_SUBSTRINGS)\n",
    "\n",
    "chunk_files = sorted(CHUNKS_DIR.glob(\"*.chunks.jsonl\"))\n",
    "eligible_files = [p for p in chunk_files if not skip_file(p)]\n",
    "assert eligible_files, \"No eligible papers after applying SKIP_SUBSTRINGS.\"\n",
    "\n",
    "num_papers = len(eligible_files)   # should be 36 for you\n",
    "base = REMAINING // num_papers\n",
    "extra = REMAINING - base * num_papers  # first 'extra' papers get +1\n",
    "per_paper_targets = {p: base for p in eligible_files}\n",
    "for p in eligible_files[:extra]:\n",
    "    per_paper_targets[p] += 1\n",
    "\n",
    "print(f\"Eligible papers: {num_papers}\")\n",
    "print(f\"Per-paper targets: base={base}, extra={extra}  → 53/54 mix\")\n",
    "print(f\"k (triplets per chunk) = {N_TRIPLETS_PER_CHUNK}\")\n",
    "print(f\"Output file → {triplets_path}\")\n",
    "\n",
    "# -----------------------\n",
    "# READ CHUNKS & BUILD WORK\n",
    "# -----------------------\n",
    "def read_chunks(file_path: Path):\n",
    "    rows = []\n",
    "    with jsonlines.open(file_path, \"r\") as r:\n",
    "        for obj in r:\n",
    "            rows.append(obj)\n",
    "    return rows\n",
    "\n",
    "work = []  # list[(paper_path, chunk_obj)]\n",
    "for cf in eligible_files:\n",
    "    need_triplets = per_paper_targets[cf]                 # 53 or 54\n",
    "    need_chunks   = math.ceil(need_triplets / N_TRIPLETS_PER_CHUNK)\n",
    "    rows = read_chunks(cf)\n",
    "    chosen = rows if len(rows) <= need_chunks else random.sample(rows, need_chunks)\n",
    "    for obj in chosen:\n",
    "        work.append((cf, obj))\n",
    "\n",
    "# cap by theoretical max chunk need\n",
    "max_chunks_needed = math.ceil(REMAINING / N_TRIPLETS_PER_CHUNK)\n",
    "if len(work) > max_chunks_needed:\n",
    "    work = work[:max_chunks_needed]\n",
    "\n",
    "print(f\"Planned chunks to process: {len(work)}  (cap: {max_chunks_needed})\")\n",
    "\n",
    "# -----------------------\n",
    "# MODEL CALL (with retry)\n",
    "# -----------------------\n",
    "@retry(stop=stop_after_attempt(4), wait=wait_exponential_jitter(min=1, max=8))\n",
    "def call_model(chunk_text: str, k: int):\n",
    "    user_prompt = f\"CHUNK:\\n\\n{chunk_text}\\n\\n---\\nPlease output exactly a JSON array of {k} triplets.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": GEN_PROMPT.format(k=k)},\n",
    "        {\"role\": \"user\",   \"content\": user_prompt},\n",
    "    ]\n",
    "    # vLLM supports Chat Completions compat\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        temperature=0.2,          # adjust if you need more variety\n",
    "        max_tokens=800,           # guardrail for long outputs\n",
    "    )\n",
    "    text = resp.choices[0].message.content\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        return data if isinstance(data, list) else []\n",
    "    except Exception:\n",
    "        # Be permissive: if malformed, return empty instead of failing the batch\n",
    "        return []\n",
    "\n",
    "def make_records(meta_chunk_obj, triplets_list):\n",
    "    out = []\n",
    "    for t in triplets_list:\n",
    "        rec = {\n",
    "            \"question\": (t.get(\"question\") or \"\").strip(),\n",
    "            \"input\":     t.get(\"input\", \"\"),\n",
    "            \"response\": (t.get(\"response\") or \"\").strip(),\n",
    "            \"meta\": meta_chunk_obj,  # carries source, chunk_id, tokens, etc.\n",
    "        }\n",
    "        if rec[\"question\"] and rec[\"response\"]:\n",
    "            out.append(rec)\n",
    "    return out\n",
    "\n",
    "# -----------------------\n",
    "# PARALLEL EXECUTION (single writer)\n",
    "# -----------------------\n",
    "written = 0\n",
    "per_source_written = collections.Counter()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex, jsonlines.open(triplets_path, \"a\") as w:\n",
    "    futures = []\n",
    "    for (src_path, chunk_obj) in work:\n",
    "        fut = ex.submit(call_model, chunk_obj[\"text\"], N_TRIPLETS_PER_CHUNK)\n",
    "        fut.meta_chunk = chunk_obj\n",
    "        futures.append(fut)\n",
    "\n",
    "    for fut in as_completed(futures):\n",
    "        triplets = fut.result()\n",
    "        recs = make_records(fut.meta_chunk, triplets)\n",
    "        for rec in recs:\n",
    "            if written >= REMAINING:\n",
    "                break\n",
    "            w.write(rec)  # single writer (main thread) → no file corruption\n",
    "            written += 1\n",
    "            per_source_written[rec[\"meta\"][\"source\"]] += 1\n",
    "        if written >= REMAINING:\n",
    "            break\n",
    "\n",
    "print(f\"[DONE] Wrote {written} triplets → {triplets_path}\")\n",
    "print(\"Per-paper tally (top 10):\")\n",
    "for k, v in per_source_written.most_common(10):\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"Total papers written:\", len(per_source_written))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69389637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
