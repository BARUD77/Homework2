{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aeea6b1",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd707059",
   "metadata": {},
   "source": [
    "You are provided with a list of technical reports about large language models (LLMs) trained by different companies. Your assignment is to convert these PDFs into a training dataset that could be used to train an LLM. <br>\n",
    "Write a code to download, extract, and combine all the PDFs into one text file.\n",
    "Write code to download all the provided PDF files,\n",
    "extract text from them, and\n",
    "Combine everything into a single text file. <br>\n",
    "Clean the text file, e.g., fix hyphenation across line breaks, remove headers/footers/page numbers, drop figure/table captions, trim “References” etc.. \n",
    "For each cleaning step, provide comments explaining why you chose to apply it. <br>\n",
    "Tokenization\n",
    "Implement a regex-based tokenizer and build your own vocabulary.\n",
    "Tokenize the same text using Byte Pair Encoding (BPE) from the tiktoken library.\n",
    "Compare the two methods and comment on the differences (e.g., handling of unknown words, vocabulary size, subword splitting). <br>\n",
    "Your own dataset and dataloader\n",
    "Prepare your own dataset of input–target sequences (using a sliding window approach).\n",
    "Implement a PyTorch DataLoader to batch the dataset for training. <br>\n",
    "Statistical Analysis\n",
    "Total documents, total tokens, average tokens per doc.\n",
    "Compare before vs after cleaning: how much text was removed. <br>\n",
    "\n",
    "You should prepare a single python notebook with your code and answers. Make sure your notebook is working without any errors before submitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7dbaa4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f2d04a1",
   "metadata": {},
   "source": [
    "# 1. Download, extract and combine all PDF files into one text file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5fa4f",
   "metadata": {},
   "source": [
    "## a. Download pdfs from urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64952c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def download_pdf(url, filename, folder=\"downloads\"):\n",
    "    # Ensure the target folder exists\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    # Build the full path\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    \n",
    "    # Download and save\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {url} (Status code: {response.status_code})\")\n",
    "\n",
    "\n",
    "# List of (title, url) for the PDFs to download\n",
    "pdf_links = [\n",
    "    (\"GPT-3\", \"https://arxiv.org/pdf/2005.14165.pdf\"),\n",
    "    (\"GPT-4\", \"https://arxiv.org/pdf/2303.08774.pdf\"),\n",
    "    (\"PaLM\", \"https://arxiv.org/pdf/2204.02311.pdf\"),\n",
    "    (\"PaLM2\", \"https://arxiv.org/pdf/2305.10403.pdf\"),\n",
    "    (\"Gemini 1.0\", \"https://arxiv.org/pdf/2312.11805.pdf\"),\n",
    "    (\"Gemini 1.5 (2024)\", \" https://arxiv.org/pdf/2403.05530.pdf\"),\n",
    "    (\"Gemma (2024)\", \" https://arxiv.org/pdf/2403.08295.pdf\"),\n",
    "    (\"Gemma 2 (2024)\", \" https://arxiv.org/pdf/2408.00118.pdf\"),\n",
    "    (\"Gemma 3\", \" https://arxiv.org/pdf/2503.19786.pdf\"),\n",
    "    (\"CodeGemma (2024)\", \" https://arxiv.org/pdf/2406.11409.pdf\"),\n",
    "    (\"RecurrentGemma (2024)\", \" https://arxiv.org/pdf/2404.07839.pdf\"),\n",
    "    (\"LLaMA (2023)\", \" https://arxiv.org/pdf/2302.13971.pdf\"),\n",
    "    (\"Llama 2 (2023)\", \" https://arxiv.org/pdf/2307.09288.pdf\"),\n",
    "    (\"Llama 3 (2024)\", \" https://arxiv.org/pdf/2407.21783.pdf\"),\n",
    "    # Mistral\n",
    "    (\"Mistral 7B (2023)\", \" https://arxiv.org/pdf/2310.06825.pdf\"),\n",
    "    (\"Mixtral of Experts 8x7B (2024)\", \" https://arxiv.org/pdf/2401.04088.pdf\"),\n",
    "    # NVIDIA\n",
    "    (\"Nemotron-4 340B Technical Report (2024)\", \" https://arxiv.org/pdf/2406.11704.pdf\"),\n",
    "    (\"NVLM 1.0 (2024)\", \" https://arxiv.org/pdf/2409.11402.pdf\"),\n",
    "    # Alibaba / Qwen series\n",
    "    (\"Qwen2 Technical Report (2024)\", \" https://arxiv.org/pdf/2407.10671.pdf\"),\n",
    "    (\"Qwen2-VL (2024)\", \" https://arxiv.org/pdf/2409.12191.pdf\"),\n",
    "    (\"Qwen2-Audio (2024)\", \" https://arxiv.org/pdf/2407.10759.pdf\"),\n",
    "    (\"Qwen2.5 Technical Report (2024)\", \" https://arxiv.org/pdf/2412.15115.pdf\"),\n",
    "    (\"Qwen2.5-VL Technical Report (2025)\", \" https://arxiv.org/pdf/2502.13923.pdf\"),\n",
    "    (\"Qwen2.5-Omni Technical Report (2025)\", \" https://arxiv.org/pdf/2503.20215.pdf\"),\n",
    "    (\"Qwen3 Technical Report (2025)\", \" https://arxiv.org/pdf/2505.09388.pdf\"),\n",
    "    # DeepSeek series\n",
    "    (\"DeepSeek-V2 (2024)\", \" https://arxiv.org/pdf/2405.04434.pdf\"),\n",
    "    (\"DeepSeek-V3 Technical Report (2024)\", \" https://arxiv.org/pdf/2412.19437.pdf\"),\n",
    "    (\"DeepSeek-R1 (2025)\", \" https://arxiv.org/pdf/2501.12948.pdf\"),\n",
    "    (\"DeepSeek-Coder (2024)\", \" https://arxiv.org/pdf/2401.14196.pdf\"),\n",
    "    # ZhipuAI\n",
    "    (\"GLM-130B (2022)\", \" https://arxiv.org/pdf/2210.02414.pdf\"),\n",
    "    # Shanghai AI Lab\n",
    "    (\"InternLM2 Technical Report (2024)\", \" https://arxiv.org/pdf/2403.17297.pdf\"),\n",
    "    (\"InternVL 2.5 (2024)\", \" https://arxiv.org/pdf/2412.05271.pdf\"),\n",
    "    # Microsoft\n",
    "    (\"Phi-3 Technical Report (2024)\", \" https://arxiv.org/pdf/2404.14219.pdf\"),\n",
    "    (\"Phi-3 Safety Post-Training (2024)\", \" https://arxiv.org/pdf/2407.13833.pdf\"),\n",
    "    # AI21\n",
    "    (\"Jamba Hybrid Transformer–Mamba (2024)\", \" https://arxiv.org/pdf/2403.19887.pdf\"),\n",
    "    # Huawei\n",
    "    (\"PanGu-Σ (2023)\", \" https://arxiv.org/pdf/2303.10845.pdf\"),\n",
    "    # 01.AI\n",
    "    (\"Yi Open Foundation Models (2024)\", \" https://arxiv.org/pdf/2403.04652.pdf\")\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# for file_name, url in pdf_links:\n",
    "#     download_pdf(url, f'{file_name}.pdf', folder=\"pdfs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba222be",
   "metadata": {},
   "source": [
    "## b. & c. Extract the text and combine into one txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bfb6e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\user\\miniconda3\\envs\\cuda_test\\lib\\site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a865b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def pdfs_to_text_with_delimiters(pdf_files, output_txt):\n",
    "    \"\"\"\n",
    "    Extract text from PDFs and add clear delimiters between papers.\n",
    "    \"\"\"\n",
    "    with open(output_txt, 'w', encoding='utf-8') as outfile:\n",
    "        for i, pdf_file in enumerate(pdf_files):\n",
    "            # Add paper delimiter (skip for first paper)\n",
    "            if i > 0:\n",
    "                outfile.write('\\n' + '='*80 + '\\n')\n",
    "                outfile.write(f'PAPER_DELIMITER_{i}\\n')\n",
    "                outfile.write('='*80 + '\\n\\n')\n",
    "            \n",
    "            reader = PdfReader(pdf_file)\n",
    "            \n",
    "            # Extract text from each page\n",
    "            for page_num, page in enumerate(reader.pages):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    outfile.write(f\"\\n[PAGE_{page_num+1}]\\n\" + text + '\\f')\n",
    "            \n",
    "    print(f'Text with delimiters merged into: {output_txt}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec2ad4",
   "metadata": {},
   "source": [
    "# 3. Clean the text file, e.g., fix hyphenation across line breaks, remove headers/footers/page numbers, drop figure/table captions, trim “References” etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93410a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with delimiters merged into: raw_extracted_text.txt\n",
      "Cleaning completed!\n",
      "Original: 4,811,565 chars → Final: 2,791,367 chars\n",
      "Removed: 2,020,198 chars (42.0%)\n",
      "Process complete! Check 'cleaned_text.txt' for the final output.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def clean_text_comprehensive(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning pipeline for PDF-extracted text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the input text\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    original_length = len(text)\n",
    "    \n",
    "    # Step 1: Fix hyphenation across line breaks\n",
    "    # Why: PDFs often break words across lines with hyphens, creating artificial word splits\n",
    "    text = re.sub(r'([a-zA-Z])-\\s*\\n\\s*([a-zA-Z])', r'\\1\\2', text)\n",
    "    \n",
    "    # Step 2: Remove page markers (our debug markers)\n",
    "    # Why: These are artifacts from our extraction process, not content\n",
    "    text = re.sub(r'\\[PAGE_\\d+\\]', '', text)\n",
    "    \n",
    "    # Step 3: Remove form feed characters\n",
    "    # Why: These are page break markers that don't add semantic value\n",
    "    text = text.replace('\\f', ' ')\n",
    "    \n",
    "    # Step 4: Remove common headers and footers\n",
    "    # Why: Headers/footers are repetitive metadata that don't contribute to learning\n",
    "    header_patterns = [\n",
    "        r'^\\d+\\s*$',  # Page numbers on separate lines\n",
    "        r'^Page \\d+ of \\d+\\s*$',  # \"Page X of Y\"\n",
    "        r'^arXiv:\\d+\\.\\d+v\\d+.*$',  # arXiv identifiers\n",
    "        r'^Preprint.*$',\n",
    "        r'^Draft.*$',\n",
    "        r'^Submitted to.*$',\n",
    "        r'^Published in.*$',\n",
    "        r'^\\w+\\s+et al\\.\\s*$',  # Author names as headers\n",
    "    ]\n",
    "    \n",
    "    footer_patterns = [\n",
    "        r'^\\d+\\s*$',  # Standalone page numbers\n",
    "        r'^©.*\\d{4}.*$',  # Copyright notices\n",
    "        r'^Manuscript.*$',\n",
    "        r'^Confidential.*$',\n",
    "    ]\n",
    "    \n",
    "    # Remove headers and footers line by line\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "        is_header_footer = False\n",
    "        \n",
    "        # Check against header patterns\n",
    "        for pattern in header_patterns + footer_patterns:\n",
    "            if re.match(pattern, line_stripped, re.IGNORECASE):\n",
    "                is_header_footer = True\n",
    "                break\n",
    "        \n",
    "        # Also remove very short lines that are likely page numbers\n",
    "        if len(line_stripped) <= 2 and line_stripped.isdigit():\n",
    "            is_header_footer = True\n",
    "            \n",
    "        if not is_header_footer:\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    text = '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    # Step 5: Remove figure and table captions\n",
    "    # Why: Captions often reference figures/tables not present in text, adding noise\n",
    "    caption_patterns = [\n",
    "        r'Figure \\d+[:\\.].*?(?=\\n\\n|\\n[A-Z]|\\n\\d|\\Z)',\n",
    "        r'Fig\\. \\d+[:\\.].*?(?=\\n\\n|\\n[A-Z]|\\n\\d|\\Z)',\n",
    "        r'Table \\d+[:\\.].*?(?=\\n\\n|\\n[A-Z]|\\n\\d|\\Z)',\n",
    "        r'Algorithm \\d+[:\\.].*?(?=\\n\\n|\\n[A-Z]|\\n\\d|\\Z)',\n",
    "        r'Listing \\d+[:\\.].*?(?=\\n\\n|\\n[A-Z]|\\n\\d|\\Z)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in caption_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # Step 6: Trim references sections\n",
    "    # Why: Reference lists are formatted metadata, not natural language content\n",
    "    papers = text.split('='*80)\n",
    "    cleaned_papers = []\n",
    "    \n",
    "    for paper in papers:\n",
    "        if not paper.strip():\n",
    "            continue\n",
    "            \n",
    "        # Look for references section\n",
    "        ref_patterns = [\n",
    "            r'\\n\\s*References\\s*\\n.*',\n",
    "            r'\\n\\s*REFERENCES\\s*\\n.*',\n",
    "            r'\\n\\s*Bibliography\\s*\\n.*',\n",
    "            r'\\n\\s*BIBLIOGRAPHY\\s*\\n.*',\n",
    "        ]\n",
    "        \n",
    "        paper_cleaned = paper\n",
    "        for pattern in ref_patterns:\n",
    "            # Remove everything from \"References\" to end of paper\n",
    "            match = re.search(pattern, paper_cleaned, re.DOTALL | re.IGNORECASE)\n",
    "            if match:\n",
    "                paper_cleaned = paper_cleaned[:match.start()]\n",
    "                break\n",
    "        \n",
    "        cleaned_papers.append(paper_cleaned)\n",
    "    \n",
    "    text = ('='*80).join(cleaned_papers)\n",
    "    \n",
    "    # Step 7: Remove acknowledgments\n",
    "    # Why: Acknowledgments are social metadata, not technical content\n",
    "    ack_patterns = [\n",
    "        r'\\n\\s*Acknowledgments?\\s*\\n.*?(?=\\n\\s*[A-Z][a-z]+\\s*\\n|\\Z)',\n",
    "        r'\\n\\s*ACKNOWLEDGMENTS?\\s*\\n.*?(?=\\n\\s*[A-Z][a-z]+\\s*\\n|\\Z)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in ack_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Step 8: Clean up excessive whitespace\n",
    "    # Why: Multiple consecutive spaces/newlines don't add information and waste tokens\n",
    "    text = re.sub(r' +', ' ', text)  # Multiple spaces to single space\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines to double newline\n",
    "    \n",
    "    # Remove leading/trailing whitespace from lines\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    text = '\\n'.join(lines)\n",
    "    text = text.strip()  # Remove empty lines at start and end\n",
    "    \n",
    "    # Write cleaned text\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    print(f\"Cleaning completed!\")\n",
    "    print(f\"Original: {original_length:,} chars → Final: {len(text):,} chars\")\n",
    "    print(f\"Removed: {original_length - len(text):,} chars ({((original_length - len(text))/original_length)*100:.1f}%)\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "pdf_list = [f'pdfs\\{file_name}.pdf' for file_name, url in pdf_links]\n",
    "\n",
    "# Step 1: Extract PDFs with delimiters\n",
    "pdfs_to_text_with_delimiters(pdf_list, 'raw_extracted_text.txt')\n",
    "\n",
    "# Step 2: Clean the text\n",
    "cleaned_text = clean_text_comprehensive('raw_extracted_text.txt', 'cleaned_text.txt')\n",
    "\n",
    "print(\"Process complete! Check 'cleaned_text.txt' for the final output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf3ae9",
   "metadata": {},
   "source": [
    "# 5. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c4560",
   "metadata": {},
   "source": [
    "### Implementing regex based tokenizer and building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ddb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.tokens2ids = {token:id for id,token in enumerate(vocab)}\n",
    "        self.ids2tokens = {id:token for id,token in enumerate(vocab)}\n",
    "\n",
    "    # encode function turns text into token IDs\n",
    "    def encode(self, text):\n",
    "        text2tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        text2tokens = [\n",
    "            item.strip() for item in text2tokens if item.strip()\n",
    "        ]\n",
    "\n",
    "        text2tokens = [\n",
    "            item if item in self.tokens2ids\n",
    "            else \"<|unk|>\" for item in text2tokens\n",
    "            ]\n",
    "\n",
    "        ids = [self.tokens2ids[s] for s in text2tokens]\n",
    "        return ids\n",
    "\n",
    "    # decode function turns token IDs back into text\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.ids2tokens[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1fc521",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbdeea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Language', 'Models', 'are', 'Few-Shot', 'Learners', 'Tom', 'B', '.', 'Brown\\x03Benjamin', 'Mann\\x03Nick', 'Ryder\\x03Melanie', 'Subbiah\\x03', 'Jared', 'KaplanyPrafulla', 'Dhariwal', 'Arvind', 'Neelakantan', 'Pranav', 'Shyam', 'Girish', 'Sastry', 'Amanda', 'Askell', 'Sandhini', 'Agarwal', 'Ariel', 'Herbert-Voss', 'Gretchen', 'Krueger', 'Tom']\n",
      "Total Number of Tokens in \"cleaned_2.txt\" are  555770\n",
      "Vocabulary size: 33938\n",
      "['\\x00', '\\x001', '\\x0013%', '\\x002', '\\x00erce-looking', '\\x00roEht', '\\x01', '\\x01Distributed', '\\x01Natural', '\\x01xV', '\\x0210\\x005', '\\x023\\x02m\\x02t', '\\x02PUE', '\\x02smaller', '\\x03\\x06d', '\\x03Davinci', '\\x03Equal', '\\x03l', '\\x03t\\x00l', '\\x06']\n"
     ]
    }
   ],
   "source": [
    "with open(\"cleaned_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cleaned_text = f.read()\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', cleaned_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])\n",
    "print('Total Number of Tokens in \"cleaned_2.txt\" are ', len(preprocessed))\n",
    "\n",
    "# Building the vocabulary\n",
    "vocab = sorted(set(preprocessed))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocabulary size:\",vocab_size)\n",
    "print(vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccd9500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10689, 11825, 18428, 8340, 10764, 15618, 5968, 441, 6418, 11376, 14068, 15018, 10018, 10270, 7650, 5826, 12179, 13087, 14656, 8912, 14289, 5639, 5852, 14264, 5495, 5796, 9326, 9028, 10470, 15618, 9317, 13913, 6837, 5461, 13654, 7391, 11090, 441, 16962, 10042, 16563, 6934, 16495, 6886, 9343, 11416, 6807, 8064, 14672, 11449, 10913, 14387, 9015, 6187, 6828, 9975, 6913, 6886, 6196, 14249, 11514, 5562, 13619, 9595, 15090, 7400, 5657, 12486, 5398, 13747, 31809, 23154, 20855, 29683, 22771, 26146, 24963, 12038, 30001, 18131, 18894, 19211, 27124, 26146, 17597, 24472, 20381, 26060, 30128, 22532, 19211, 33543, 26146, 17597, 29286, 29989, 441, 16425, 30819, 29991]\n"
     ]
    }
   ],
   "source": [
    "# Encoding and decoding text with our regex based tokenizer and vocabulary\n",
    "regextokenizer = SimpleTokenizer(vocab)\n",
    "\n",
    "regex_token_integers = regextokenizer.encode(cleaned_text)\n",
    "print(regex_token_integers[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c0591b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Models are Few-Shot Learners Tom B. Brown\u0003Benjamin Mann\u0003Nick Ryder\u0003Melanie Subbiah\u0003 Jared KaplanyPrafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell Sandhini Agarwal Ariel Herbert-Voss Gretchen Krueger Tom Henighan Rewon Child Aditya Ramesh Daniel M. Ziegler Jeffrey Wu Clemens Winter Christopher Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray Benjamin Chess Jack Clark Christopher Berner Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei OpenAI Abstract Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic\n"
     ]
    }
   ],
   "source": [
    "print(regextokenizer.decode(regex_token_integers[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183531d",
   "metadata": {},
   "source": [
    "### BPE tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4774d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/hermon/miniconda3/envs/dl/lib/python3.10/site-packages (from tiktoken) (2025.9.18)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/hermon/miniconda3/envs/dl/lib/python3.10/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/hermon/miniconda3/envs/dl/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hermon/miniconda3/envs/dl/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hermon/miniconda3/envs/dl/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hermon/miniconda3/envs/dl/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n",
      "Downloading tiktoken-0.12.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9778a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32065, 32329, 389, 20463, 12, 28512, 8010, 2741, 198, 13787, 347, 13, 4373, 191, 11696, 13337, 20291, 191, 23609, 40238, 191, 21102, 34166, 3834, 65, 9520, 191, 198, 41, 1144, 11611, 489, 1092, 47, 430, 12853, 64, 20529, 2743, 16783, 943, 50172, 3169, 417, 461, 415, 272, 1736, 272, 615, 36838, 321, 23837, 680, 311, 459, 563, 198, 5840, 5282, 1081, 17164, 3837, 71, 5362, 2449, 283, 16783, 33364, 28648, 12, 53, 793, 35877, 6607, 13685, 518, 1362, 4186, 6752, 394, 272, 198, 30003, 261, 5932, 1215, 414, 64, 371, 1047, 71, 7806, 337, 13, 1168, 15702, 1754, 19627, 18027]\n",
      "Language Models are Few-Shot Learners\n",
      "Tom B. Brown\u0003Benjamin Mann\u0003Nick Ryder\u0003Melanie Subbiah\u0003\n",
      "Jared K\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "integers = bpe_tokenizer.encode(cleaned_text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers[:100])\n",
    "\n",
    "strings = bpe_tokenizer.decode(integers)\n",
    "\n",
    "print(strings[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa15aba2",
   "metadata": {},
   "source": [
    "### Comparison of regex and bpe tokenizer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4e7df90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regex_total_tokens': 555770,\n",
       " 'regex_vocab_size': 33938,\n",
       " 'bpe_total_tokens': 777055,\n",
       " 'bpe_vocab_size': 20646}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "total_regex_tokens = len(regex_token_integers)\n",
    "regex_vocab_size = len(set(regex_token_integers))\n",
    "total_bpe_tokens = len(integers)\n",
    "bpe_vocab_size = len(set(integers))\n",
    "\n",
    "\n",
    "comparison = {\n",
    "    \"regex_total_tokens\": total_regex_tokens,\n",
    "    \"regex_vocab_size\": regex_vocab_size,\n",
    "    \"bpe_total_tokens\": total_bpe_tokens,\n",
    "    \"bpe_vocab_size\": bpe_vocab_size\n",
    "}\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a4b12",
   "metadata": {},
   "source": [
    "# Key Differences between Regex Tokenizer and BPE Tokenizer\n",
    "\n",
    "### 1. Handling of Unknown Words\n",
    "\n",
    "- **Regex Tokenizer:** Maps unknown words (like \"bioinformatics\", \"neologisms\") to a single `<unk>` token, causing information loss.\n",
    "\n",
    "- **BPE Tokenizer:** Breaks unknown words into meaningful subword components, preserving all information.\n",
    "\n",
    "### 2. Vocabulary Size vs Coverage\n",
    "\n",
    "- **Regex:** Larger vocabularies needed to cover diverse text, but still fails on rare words.\n",
    "\n",
    "- **BPE:** Smaller, more efficient vocabularies with better coverage through subword composition.\n",
    "\n",
    "### 3. Subword Splitting Capability\n",
    "\n",
    "- **Regex:** No subword splitting - treats \"bioinformatics\" as atomic unit.\n",
    "\n",
    "- **BPE:** Intelligent splitting into morphologically meaningful parts like \"bio-in-form-a-tics\".\n",
    "\n",
    "### 4. Efficiency Trade-offs\n",
    "\n",
    "- **Regex:** Fewer tokens per sentence, computationally efficient.\n",
    "\n",
    "- **BPE:** More tokens but better semantic representation and robustness.\n",
    "\n",
    "### 5. Generalization\n",
    "\n",
    "- **Regex:** Poor - cannot handle words not seen during vocabulary building.\n",
    "\n",
    "- **BPE:** Excellent - can represent any word through subword combinations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b5a498",
   "metadata": {},
   "source": [
    "# 7. Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4210f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, context_size, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > context_size, \"Number of tokenized inputs must at least be equal to context_size+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of context_size\n",
    "        for i in range(0, len(token_ids) - context_size, stride):\n",
    "            input_chunk = token_ids[i:i + context_size]\n",
    "            target_chunk = token_ids[i + 1: i + context_size + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "494d80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, batch_size=4, context_size=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = CustomDataset(txt, tokenizer, context_size, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df200876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[32065, 32329,   389, 20463],\n",
      "        [32329,   389, 20463,    12]]), tensor([[32329,   389, 20463,    12],\n",
      "        [  389, 20463,    12, 28512]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(\n",
    "    cleaned_text, batch_size=2, context_size=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9374f5",
   "metadata": {},
   "source": [
    "# 9. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2120085d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_docs': 37,\n",
       " 'regex_total_tokens': 555770,\n",
       " 'bpe_total_tokens': 777055,\n",
       " 'avg_tokens_per_doc_regex': 15020.81081081081,\n",
       " 'avg_tokens_per_doc_bpe': 21001.486486486487,\n",
       " 'removed_char_percentage_weighted': 0.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Per-doc removals (character-level proxy)\n",
    "\n",
    "\n",
    "summary = {\n",
    "    \"total_docs\": len(pdf_links),\n",
    "    \"regex_total_tokens\": int(len(regex_token_integers)),\n",
    "    \"bpe_total_tokens\": int(len(integers)),\n",
    "    \"avg_tokens_per_doc_regex\": float((len(regex_token_integers) / len(pdf_links))),\n",
    "    \"avg_tokens_per_doc_bpe\": float((len(integers) / len(pdf_links))),\n",
    "    \"removed_char_percentage_weighted\": float(),\n",
    "}\n",
    "\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
