{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e75c332d",
      "metadata": {
        "id": "e75c332d"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tiktoken\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloning repository and mounting drive"
      ],
      "metadata": {
        "id": "KybvBQGTJbLT"
      },
      "id": "KybvBQGTJbLT"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/BARUD77/Homework2.git\n",
        "%cd Homework2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e75Vl6BRJa0r",
        "outputId": "67c421ab-64df-495c-dd4b-09e6451ef491"
      },
      "id": "e75Vl6BRJa0r",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Homework2' already exists and is not an empty directory.\n",
            "/content/Homework2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ll"
      ],
      "metadata": {
        "id": "mK1-TgRZNlVr",
        "outputId": "d1b6102b-68e9-450f-a673-a33eaed6d0e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mK1-TgRZNlVr",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4748\n",
            "-rw-r--r-- 1 root    2775 Nov  4 09:31 artifact_simulation.py\n",
            "-rw-r--r-- 1 root 2851073 Nov  4 09:31 cleaned_text.txt\n",
            "-rw-r--r-- 1 root    1381 Nov  4 09:31 dataset.py\n",
            "-rw-r--r-- 1 root    6371 Nov  4 09:31 extract_sheet_changes.py\n",
            "-rw-r--r-- 1 root    8623 Nov  4 09:31 extract_txt.py\n",
            "-rw-r--r-- 1 root   10377 Nov  4 09:31 frame_extractor.py\n",
            "drwxr-xr-x 4 root    4096 Nov  4 09:32 \u001b[0m\u001b[01;34mHomework2\u001b[0m/\n",
            "-rw-r--r-- 1 root   87694 Nov  4 09:31 Homework2.ipynb\n",
            "-rw-r--r-- 1 root   83486 Nov  4 09:31 Homework2_regex.ipynb\n",
            "-rw-r--r-- 1 root   94232 Nov  4 09:31 IFD_creation.ipynb\n",
            "-rw-r--r-- 1 root   32209 Nov  4 09:31 LLM_Homework_1_Hermon_Teklesenbet_100064487.ipynb\n",
            "-rw-r--r-- 1 root   91180 Nov  4 09:31 loss_plot.png\n",
            "-rw-r--r-- 1 root 1527298 Nov  4 09:31 merged_corpus.txt\n",
            "-rw-r--r-- 1 root    4928 Nov  4 09:31 model.py\n",
            "drwxr-xr-x 2 root    4096 Nov  4 09:39 \u001b[01;34m__pycache__\u001b[0m/\n",
            "-rw-r--r-- 1 root    7763 Nov  4 09:42 train.py\n",
            "-rw-r--r-- 1 root    8860 Nov  4 09:31 Untitled-1.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5264777e",
      "metadata": {
        "id": "5264777e"
      },
      "source": [
        "# Loading the cleaned and merged text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2396b616",
      "metadata": {
        "id": "2396b616"
      },
      "outputs": [],
      "source": [
        "with open(\"cleaned_text.txt\", \"r\") as f:\n",
        "    cleaned_text = f.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4a4d351",
      "metadata": {
        "id": "c4a4d351"
      },
      "source": [
        "# Tokenization with both regex and bpe tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f7ae87",
      "metadata": {
        "id": "f6f7ae87"
      },
      "source": [
        "### Regex tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9c7a375f",
      "metadata": {
        "id": "9c7a375f"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.tokens2ids = {tok: i for i, tok in enumerate(vocab)}\n",
        "        self.ids2tokens = {i: tok for tok, i in self.tokens2ids.items()}\n",
        "        self.unk_id = self.tokens2ids[\"<|unk|>\"]  # required\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = re.split(r\"([.,:;?_!\\\"'()\\[\\]—\\-\\s])\", text)\n",
        "        tokens = [t.strip() for t in tokens if t and t.strip()]\n",
        "        return [self.tokens2ids.get(t, self.unk_id) for t in tokens]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join(self.ids2tokens[i] for i in ids)\n",
        "        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4e020846",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e020846",
        "outputId": "4687dc40-3670-422a-957e-2e42d2631a56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size = 29916\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# 0) (optional) clean non-printables to avoid odd tokens like '\\x001'\n",
        "def strip_nonprintable(s):\n",
        "    return ''.join(ch if unicodedata.category(ch)[0] != 'C' else ' ' for ch in s)\n",
        "\n",
        "cleaned_text = strip_nonprintable(cleaned_text)\n",
        "\n",
        "# 1) tokenize the corpus once to build the base vocab\n",
        "preprocessed = re.split(r\"([.,:;?_!\\\"'()\\[\\]—\\-\\s])\", cleaned_text)\n",
        "preprocessed = [t.strip() for t in preprocessed if t and t.strip()]\n",
        "\n",
        "# 2) build vocab set from corpus (no specials yet)\n",
        "base_vocab = set(preprocessed)\n",
        "\n",
        "# 3) add specials up front in a deterministic order\n",
        "specials = [\"<|pad|>\", \"<|bos|>\", \"<|eos|>\", \"<|unk|>\"]\n",
        "# exclude any duplicates of specials from the base set\n",
        "base_vocab -= set(specials)\n",
        "\n",
        "# 4) finalize the ordered vocab: specials first, then sorted tokens\n",
        "vocab = specials + sorted(base_vocab)\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size =\", vocab_size)\n",
        "\n",
        "# 5) construct the tokenizer with this *fixed* vocab\n",
        "regextokenizer = SimpleTokenizer(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "461dd3b6",
      "metadata": {
        "id": "461dd3b6"
      },
      "source": [
        "# Preparing the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "147c78ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "147c78ab",
        "outputId": "69b0939f-ae0b-4f47-9f75-65e7118f25ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in the model (Millions): 131.175936\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from model import GPTModel\n",
        "\n",
        "gpt2=False\n",
        "vocab_size = vocab_size\n",
        "tokenizer = regextokenizer\n",
        "\n",
        "GPT_CONFIG = {\n",
        "\"vocab_size\": vocab_size, # Vocabulary size\n",
        "\"context_length\": 256, # Context length\n",
        "\"emb_dim\": 768, # Embedding dimension\n",
        "\"n_heads\": 12, # Number of attention heads\n",
        "\"n_layers\": 12, # Number of layers\n",
        "\"drop_rate\": 0.1, # Dropout rate\n",
        "\"qkv_bias\": False # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "model = GPTModel(GPT_CONFIG)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters in the model (Millions): {total_params/1_000_000}\")\n",
        "print(\"-\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "24483e88",
      "metadata": {
        "id": "24483e88"
      },
      "outputs": [],
      "source": [
        "from dataset import create_dataloader_v1\n",
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(cleaned_text))\n",
        "train_data = cleaned_text[:split_idx]\n",
        "val_data = cleaned_text[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    gpt2,\n",
        "    tokenizer,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG[\"context_length\"],\n",
        "    stride=GPT_CONFIG[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    gpt2,\n",
        "    tokenizer,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG[\"context_length\"],\n",
        "    stride=GPT_CONFIG[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "421200a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "421200a5",
        "outputId": "d5ce11f1-ef09-4823-e316-db2f900f2bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Characters : 2506253\n",
            "Train Tokens : 537969\n",
            "====================================================================================================\n",
            "Validation Characters: 278473\n",
            "Validation Tokens: 61105\n",
            "====================================================================================================\n",
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "====================================================================================================\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ],
      "source": [
        "total_train_characters = len(train_data)\n",
        "total_train_tokens = len(tokenizer.encode(train_data))\n",
        "\n",
        "print(\"Train Characters :\", total_train_characters)\n",
        "print(\"Train Tokens :\", total_train_tokens)\n",
        "\n",
        "print(\"=\"*100)\n",
        "\n",
        "total_val_characters = len(val_data)\n",
        "total_val_tokens = len(tokenizer.encode(val_data))\n",
        "\n",
        "print(\"Validation Characters:\", total_val_characters)\n",
        "print(\"Validation Tokens:\", total_val_tokens)\n",
        "\n",
        "print(\"=\"*100)\n",
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "    break\n",
        "print(\"=\"*100)\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e7b9716",
      "metadata": {
        "id": "1e7b9716"
      },
      "source": [
        "# Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4571e01b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "4571e01b",
        "outputId": "55bfcb36-a5b0-45c0-e974-c8387823da05"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-333741209.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtarget_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtokens_per_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m train_losses, val_losses, tokens_seen = train_model_simple(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;31m# large cap; early stopping will stop earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Homework2/train.py\u001b[0m in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, gpt2, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer, early_stop, patience, min_delta, save_path, use_plateau_lr, lr_factor, lr_patience, min_lr, max_tokens_seen)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# safe default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Homework2/train.py\u001b[0m in \u001b[0;36mcalc_loss_batch\u001b[0;34m(input_batch, target_batch, model, device)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Homework2/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in_idx)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrf_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from train import train_model_simple\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=0.1)\n",
        "\n",
        "# estimate a token budget: ~5 epochs worth\n",
        "B, T = next(iter(train_loader))[0].shape  # (B, T)\n",
        "tokens_per_epoch = len(train_loader) * B * T\n",
        "target_tokens = 30 * tokens_per_epoch\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, gpt2, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=30,                 # large cap; early stopping will stop earlier\n",
        "    eval_freq=200,                  # evaluate every N steps (tune to your speed)\n",
        "    eval_iter=5,\n",
        "    start_context=\"Finally, given the broad spectrum of capabilities displayed by GPT-3\",\n",
        "    tokenizer=tokenizer,\n",
        "    # early stopping knobs\n",
        "    early_stop=True, patience=5, min_delta=1e-3,\n",
        "    save_path=\"checkpoints/regex/best.pt\",\n",
        "    use_plateau_lr=True, lr_factor=0.5, lr_patience=2, min_lr=1e-5,\n",
        "    max_tokens_seen=target_tokens\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad92f0c",
      "metadata": {
        "id": "bad92f0c"
      },
      "source": [
        "# Plotting training and validation losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d54f854",
      "metadata": {
        "id": "1d54f854"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_losses(tokens_seen, train_losses, val_losses, save_path=None):\n",
        "    \"\"\"\n",
        "    tokens_seen: list from train_model_simple (logged at each eval)\n",
        "    train_losses, val_losses: same length as tokens_seen\n",
        "    \"\"\"\n",
        "    if not (len(tokens_seen) == len(train_losses) == len(val_losses)):\n",
        "        raise ValueError(\"tokens_seen, train_losses, val_losses must have same length\")\n",
        "\n",
        "    plt.figure(figsize=(7,4.5))\n",
        "    plt.plot(tokens_seen, train_losses, label=\"Train loss\")\n",
        "    plt.plot(tokens_seen, val_losses, label=\"Val loss\")\n",
        "    plt.xlabel(\"Tokens seen\")\n",
        "    plt.ylabel(\"Cross-entropy loss\")\n",
        "    plt.title(\"Training/Validation Loss vs Tokens\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "plot_losses(tokens_seen, train_losses, val_losses, save_path=\"loss_plot.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca240d21",
      "metadata": {
        "id": "ca240d21"
      },
      "source": [
        "# Loading trained model later"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}